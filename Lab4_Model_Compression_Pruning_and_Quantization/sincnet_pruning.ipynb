{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iUJyBlui3a5A",
        "outputId": "ccde5181-ac99-4ef0-84f7-3cb5e2f1ec9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.0\n",
            "  Downloading numpy-1.21.0.zip (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy: filename=numpy-1.21.0-cp310-cp310-linux_x86_64.whl size=15985728 sha256=ccb65d6aeb0df15f47b4717c9312a661e14a325dc8555428a351555a9ea23b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/61/d1/ccc2cd557b39e127ad98a392d9558f3c5dda28764b7f54b2f5\n",
            "Successfully built numpy\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.21.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.21.0 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.21.0 which is incompatible.\n",
            "astropy 6.1.4 requires numpy>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "bigframes 1.25.0 requires numpy>=1.24.0, but you have numpy 1.21.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.21.0 which is incompatible.\n",
            "contourpy 1.3.0 requires numpy>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "flax 0.8.5 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "geopandas 1.0.1 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.21.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.21.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.21.0 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.21.0 which is incompatible.\n",
            "ml-dtypes 0.4.1 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "nibabel 5.3.2 requires numpy>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 1.21.0 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.21.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-contrib-python 4.10.0.84 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-python 4.10.0.84 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "opencv-python-headless 4.10.0.84 requires numpy>=1.21.2; python_version >= \"3.10\", but you have numpy 1.21.0 which is incompatible.\n",
            "pandas 2.2.2 requires numpy>=1.22.4; python_version < \"3.11\", but you have numpy 1.21.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.21.0 which is incompatible.\n",
            "plotnine 0.14.0 requires numpy>=1.23.5, but you have numpy 1.21.0 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.21.0 which is incompatible.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 1.21.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.21.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.21.0 which is incompatible.\n",
            "tensorstore 0.1.67 requires numpy>=1.22.0, but you have numpy 1.21.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.21.0 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5a145d11eb9d445989e1d0a0b3a0db50"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZhwHsPoyjHap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnePnPzb2rK4"
      },
      "source": [
        "# Pruning -> Fine-tune\n",
        "\n",
        "1. Sort the weight of Batchnorm1d.\n",
        "2. Remove the channel of Conv1d if the weight of Batchnorm1d before this channel is smaller than threshold.\n",
        "3. Fine-tine model to recover accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDI_5WWn2rK5"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkgWcy9D2rK6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from speech_command_dataset import SpeechCommandDataset\n",
        "from torchvision.transforms import Compose\n",
        "import torchvision.models as models\n",
        "import model\n",
        "\n",
        "# from apex import amp\n",
        "\n",
        "import os,time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M25Z8hDz2rK6"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FuzSBd72rK6"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "training_params = {\"batch_size\": BATCH_SIZE,\n",
        "                       \"shuffle\": True,\n",
        "                       \"drop_last\": False,\n",
        "                       \"num_workers\": 1}\n",
        "\n",
        "testing_params = {\"batch_size\": BATCH_SIZE,\n",
        "                       \"shuffle\": False,\n",
        "                       \"drop_last\": False,\n",
        "                       \"num_workers\": 1}\n",
        "\n",
        "train_set = SpeechCommandDataset()\n",
        "train_loader = DataLoader(train_set, **training_params)\n",
        "\n",
        "test_set = SpeechCommandDataset(is_training=False)\n",
        "test_loader = DataLoader(test_set, **testing_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1l6zUga2rK6",
        "outputId": "018b8dae-d654-43e8-9bdc-2cda7e63867f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> loading checkpoint './Checkpoint/SincNet_best.pth.tar'\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "net = model.SincNet().cuda()\n",
        "model_path = '/content/drive/MyDrive/DL_lab4/Checkpoint/SincNet_best.pth.tar'\n",
        "# you could load the model after pruning and fine-tune to prune again\n",
        "# model_path = './Checkpoint/SincNet_finetune.pth.tar'\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
        "    checkpoint = torch.load(model_path)\n",
        "#     print(checkpoint)\n",
        "    net.load_state_dict(checkpoint['state_dict'])\n",
        "else:\n",
        "    print(\"=> no checkpoint found at '{}'\".format(model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OmgrQ1v2rK7",
        "outputId": "5c9d7f60-b4a0-44ed-c717-36634db85fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before pruning\n",
            "Validation loss: 0.1473 Validation accuracy: 95.95\n"
          ]
        }
      ],
      "source": [
        "print('Before pruning')\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "total_val_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "batch_num = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for audios, labels in test_loader:\n",
        "    audios = audios.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = net(audios)\n",
        "    loss = loss_func(outputs, labels)\n",
        "    total_val_loss += loss.item()\n",
        "    batch_num += 1\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "val_loss = total_val_loss / batch_num\n",
        "val_acc = 100.0 * float(correct) / float(total)\n",
        "\n",
        "\n",
        "print('Validation loss: %.4f' % val_loss,'Validation accuracy: %.2f' % val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PhBJ_h2rK7"
      },
      "source": [
        "## Start Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UgrsBAU2rK7"
      },
      "outputs": [],
      "source": [
        "# you can choose your pruning rate\n",
        "pruning_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1vrD2kN2rK7",
        "outputId": "85c2fa1f-a2f0-4125-8ac6-ebe129eef3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameter: 0.27M\n"
          ]
        }
      ],
      "source": [
        "# extract the weight of BatchNorm1d layer and sort them\n",
        "\n",
        "total_par = sum([param.nelement() for param in net.parameters()])\n",
        "print(\"Number of parameter: %.2fM\" % (total_par/1e6))\n",
        "\n",
        "total = 0\n",
        "\n",
        "for m in net.modules():\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        total += m.weight.data.shape[0]\n",
        "\n",
        "bn = torch.zeros(total)\n",
        "index = 0\n",
        "for m in net.modules():\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        size = m.weight.data.shape[0]\n",
        "#         print(size)\n",
        "        bn[index:(index+size)] = m.weight.data.abs().clone()\n",
        "\n",
        "        index += size\n",
        "\n",
        "y, i = torch.sort(bn)\n",
        "thre_index = int(total * pruning_rate)\n",
        "thre = y[thre_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ5vn--j2rK7",
        "outputId": "02d009ac-460a-41bd-8343-1c5e43f374cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer index: 4 \t total channel: 40 \t remaining channel: 37\n",
            "layer index: 11 \t total channel: 256 \t remaining channel: 243\n",
            "layer index: 17 \t total channel: 256 \t remaining channel: 208\n",
            "layer index: 23 \t total channel: 256 \t remaining channel: 239\n",
            "layer index: 29 \t total channel: 256 \t remaining channel: 214\n",
            "layer index: 35 \t total channel: 160 \t remaining channel: 160\n",
            "Pre-processing Successful!\n"
          ]
        }
      ],
      "source": [
        "# record the renaming weight\n",
        "pruned = 0\n",
        "cfg = []\n",
        "cfg_mask = []\n",
        "for k, m in enumerate(net.modules()):\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        weight_copy = m.weight.data.abs().clone().cpu()\n",
        "\n",
        "        # if weight larger than threshold\n",
        "        mask = weight_copy.gt(thre).float().cuda()\n",
        "\n",
        "        # pruning number\n",
        "        pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
        "        m.weight.data.mul_(mask)\n",
        "        m.bias.data.mul_(mask)\n",
        "\n",
        "        cfg.append(int(torch.sum(mask)))\n",
        "        cfg_mask.append(mask.clone())\n",
        "        print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
        "\n",
        "    elif isinstance(m, nn.AvgPool1d):\n",
        "        cfg.append('P')\n",
        "\n",
        "\n",
        "# print('cfg',cfg)\n",
        "pruned_ratio = pruned/total\n",
        "\n",
        "print('Pre-processing Successful!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysNqmtnG2rK8"
      },
      "outputs": [],
      "source": [
        "new_module = model.SincNet(cfg).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiOaRZWA2rK8",
        "outputId": "76fec71d-2262-4b3a-939b-ddb2dade1983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In shape: 37, Out shape 37.\n",
            "In shape: 37, Out shape 243.\n",
            "In shape: 243, Out shape 243.\n",
            "In shape: 243, Out shape 208.\n",
            "In shape: 208, Out shape 208.\n",
            "In shape: 208, Out shape 239.\n",
            "In shape: 239, Out shape 239.\n",
            "In shape: 239, Out shape 214.\n",
            "In shape: 214, Out shape 214.\n",
            "In shape: 214, Out shape 160.\n"
          ]
        }
      ],
      "source": [
        "old_modules = list(net.modules())\n",
        "new_modules = list(new_module.modules())\n",
        "layer_id_in_cfg = 0\n",
        "start_mask = torch.ones(1)\n",
        "end_mask = cfg_mask[layer_id_in_cfg]\n",
        "conv_count = 0\n",
        "\n",
        "for layer_id in range(len(old_modules)):\n",
        "    m0 = old_modules[layer_id]\n",
        "    m1 = new_modules[layer_id]\n",
        "    if isinstance(m0, nn.BatchNorm1d):\n",
        "\n",
        "\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "\n",
        "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "        layer_id_in_cfg += 1\n",
        "        start_mask = end_mask.clone()\n",
        "        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            end_mask = cfg_mask[layer_id_in_cfg]\n",
        "    elif isinstance(m0, nn.Conv1d):\n",
        "        if isinstance(old_modules[layer_id-4], nn.BatchNorm1d) or isinstance(old_modules[layer_id-3], nn.BatchNorm1d) or isinstance(old_modules[layer_id+2], nn.BatchNorm1d):\n",
        "            # This convers the convolutions in the residual block.\n",
        "            conv_count += 1\n",
        "\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "\n",
        "\n",
        "            if conv_count % 2 != 0:\n",
        "                w1 = m0.weight.data[idx0.tolist(), :, :].clone()\n",
        "                print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx0.size))\n",
        "            else:\n",
        "                w1 = m0.weight.data[:, idx0.tolist(), :].clone()\n",
        "                w1 = w1[idx1.tolist(), :, :].clone()\n",
        "                print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "\n",
        "\n",
        "            m1.weight.data = w1.clone()\n",
        "\n",
        "            continue\n",
        "\n",
        "        # We need to consider the case where there are downsampling convolutions.\n",
        "        # For these convolutions, we just copy the weights.\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "    elif isinstance(m0, nn.Linear):\n",
        "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "        if idx0.size == 1:\n",
        "            idx0 = np.resize(idx0, (1,))\n",
        "\n",
        "        m1.weight.data = m0.weight.data[:, idx0].clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "\n",
        "# print(cfg)\n",
        "torch.save({'cfg': cfg, 'state_dict': new_module.state_dict()}, os.path.join('./Checkpoint', 'SincNet_prune.pth.tar'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ0G0iWz2rK8"
      },
      "source": [
        "## Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7YFRlQH2rK8",
        "outputId": "32e04598-1ee4-4cb8-8474-6ebbb81a98da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SincNet(\n",
            "  (sincconv): _Layer(\n",
            "    (conv0): SincConv1d()\n",
            "    (logabs): LogAbs()\n",
            "    (bn): BatchNorm1d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "  )\n",
            "  (features): ModuleList(\n",
            "    (0): _Layer(\n",
            "      (conv0): Conv1d(37, 37, kernel_size=(25,), stride=(2,), groups=37)\n",
            "      (conv1): Conv1d(37, 243, kernel_size=(1,), stride=(1,))\n",
            "      (relu): ReLU()\n",
            "      (bn): BatchNorm1d(243, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    )\n",
            "    (1): _Layer(\n",
            "      (conv0): Conv1d(243, 243, kernel_size=(9,), stride=(1,), groups=243)\n",
            "      (conv1): Conv1d(243, 208, kernel_size=(1,), stride=(1,))\n",
            "      (relu): ReLU()\n",
            "      (bn): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    )\n",
            "    (2): _Layer(\n",
            "      (conv0): Conv1d(208, 208, kernel_size=(15,), stride=(1,), groups=208)\n",
            "      (conv1): Conv1d(208, 239, kernel_size=(1,), stride=(1,))\n",
            "      (relu): ReLU()\n",
            "      (bn): BatchNorm1d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    )\n",
            "    (3): _Layer(\n",
            "      (conv0): Conv1d(239, 239, kernel_size=(9,), stride=(1,), groups=239)\n",
            "      (conv1): Conv1d(239, 214, kernel_size=(1,), stride=(1,))\n",
            "      (relu): ReLU()\n",
            "      (bn): BatchNorm1d(214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    )\n",
            "    (4): _Layer(\n",
            "      (conv0): Conv1d(214, 214, kernel_size=(9,), stride=(1,), groups=214)\n",
            "      (conv1): Conv1d(214, 160, kernel_size=(1,), stride=(1,))\n",
            "      (relu): ReLU()\n",
            "      (bn): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    )\n",
            "  )\n",
            "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
            "  (fc): Linear(in_features=160, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load('./Checkpoint/SincNet_prune.pth.tar')\n",
        "net = model.SincNet(cfg=checkpoint['cfg'])\n",
        "net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "net.cuda()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YNv68Bx2rK8",
        "outputId": "190d7818-e459-4c90-8a63-2c9ebb18f489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before fine-tune\n",
            "Validation loss: 17.7283 Validation accuracy: 21.50\n"
          ]
        }
      ],
      "source": [
        "print('Before fine-tune')\n",
        "\n",
        "total_val_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "batch_num = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for audios, labels in test_loader:\n",
        "    audios = audios.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = net(audios)\n",
        "    loss = loss_func(outputs, labels)\n",
        "    total_val_loss += loss.item()\n",
        "    batch_num += 1\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "val_loss = total_val_loss / batch_num\n",
        "val_acc = 100.0 * float(correct) / float(total)\n",
        "\n",
        "print('Validation loss: %.4f' % val_loss,'Validation accuracy: %.2f' % val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_JYSWuH2rK8"
      },
      "outputs": [],
      "source": [
        "EPOCH = 10\n",
        "LR = 1e-3\n",
        "Weight_decay = 1e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXoTabLS2rK8"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=Weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,  patience=15, verbose=True, eps=1e-09)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DNmVk6Z2rK8",
        "outputId": "0871812d-d280-4d88-a8ec-853f17ff71d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin fine-tune...\n",
            "22 Aug 2023 09:03:08\n",
            "Epoch:   0 |train loss: 0.4616 |train accuracy: 86.07 |train time: 54.33\n",
            "22 Aug 2023 09:04:02\n",
            "Epoch:   1 |train loss: 0.3654 |train accuracy: 88.82 |train time: 53.94\n",
            "22 Aug 2023 09:04:57\n",
            "Epoch:   2 |train loss: 0.3346 |train accuracy: 89.87 |train time: 54.12\n",
            "22 Aug 2023 09:05:51\n",
            "Epoch:   3 |train loss: 0.2982 |train accuracy: 90.93 |train time: 54.47\n",
            "22 Aug 2023 09:06:45\n",
            "Epoch:   4 |train loss: 0.2848 |train accuracy: 91.36 |train time: 54.21\n",
            "22 Aug 2023 09:07:39\n",
            "Epoch:   5 |train loss: 0.2652 |train accuracy: 91.97 |train time: 54.27\n",
            "22 Aug 2023 09:08:34\n",
            "Epoch:   6 |train loss: 0.2423 |train accuracy: 92.51 |train time: 54.34\n",
            "22 Aug 2023 09:09:29\n",
            "Epoch:   7 |train loss: 0.2268 |train accuracy: 93.12 |train time: 54.75\n",
            "22 Aug 2023 09:10:23\n",
            "Epoch:   8 |train loss: 0.2204 |train accuracy: 93.38 |train time: 54.54\n",
            "22 Aug 2023 09:11:18\n",
            "Epoch:   9 |train loss: 0.2125 |train accuracy: 93.68 |train time: 54.65\n",
            "Saving..\n"
          ]
        }
      ],
      "source": [
        "print('Begin fine-tune...')\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    net.train()\n",
        "    start_time = time.time()\n",
        "    total_train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    for step, (audios, labels) in enumerate(train_loader):\n",
        "        audios = audios.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(audios)\n",
        "\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        batch_num += 1\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    train_time = end_time - start_time\n",
        "    train_loss = total_train_loss / batch_num\n",
        "    train_acc = 100.0 * float(correct) / float(total)\n",
        "\n",
        "\n",
        "\n",
        "    print(time.strftime(\"%d %b %Y %H:%M:%S\", time.localtime()))\n",
        "    print('Epoch: %3d' % epoch, '|train loss: %.4f' % train_loss, '|train accuracy: %.2f' % train_acc,\n",
        "          '|train time: %.2f' % train_time)\n",
        "\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "torch.save({'cfg': None, 'state_dict': net.state_dict()}, os.path.join('./Checkpoint', 'SincNet_finetune.pth.tar'))\n",
        "print('Saving..')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICKyT2z12rK8",
        "outputId": "9ee02a35-db11-43f3-e284-e72c66c75f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameter: 0.21M\n"
          ]
        }
      ],
      "source": [
        "total_par = sum([param.nelement() for param in net.parameters()])\n",
        "print(\"Number of parameter: %.2fM\" % (total_par/1e6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDeM74mV2rK9",
        "outputId": "68864f34-2187-448f-d90f-b98142ae9b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After fine-tune\n",
            "Validation loss: 0.2430 Validation accuracy: 93.62\n"
          ]
        }
      ],
      "source": [
        "print('After fine-tune')\n",
        "\n",
        "total_val_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "batch_num = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for audios, labels in test_loader:\n",
        "    audios = audios.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    outputs = net(audios)\n",
        "    loss = loss_func(outputs, labels)\n",
        "    total_val_loss += loss.item()\n",
        "    batch_num += 1\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "\n",
        "val_loss = total_val_loss / batch_num\n",
        "val_acc = 100.0 * float(correct) / float(total)\n",
        "\n",
        "print('Validation loss: %.4f' % val_loss,'Validation accuracy: %.2f' % val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqTNUEwx2rK9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}