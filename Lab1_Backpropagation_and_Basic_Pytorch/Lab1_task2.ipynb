{"cells":[{"cell_type":"markdown","metadata":{"id":"86CvAQ3yMIZk"},"source":["# Just an example.You can alter sample code anywhere."]},{"cell_type":"markdown","metadata":{"id":"E78T_6PFWpjL"},"source":["## Mount your google drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1727059179019,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"sx47B97qMNwv","outputId":"6feab4ba-af1d-4267-8fb3-0684175b7161"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1727059179019,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"erMgvx83MUrm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e16e1ad-8d86-415c-9a6d-fbe1d464f13c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DL_Lab1\n"]}],"source":["# You need to modify this part to the directory where your code is located\n","%cd \"/content/drive/MyDrive/DL_Lab1/\""]},{"cell_type":"markdown","metadata":{"id":"fHJbvA_B47t9"},"source":["## Import packages"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4999,"status":"ok","timestamp":1727059184014,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"fv1K7EfGMIZm","scrolled":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1727059184014,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"qr3ieyG9MIZn"},"outputs":[],"source":["#Fix the random seed\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"huwyyDbJMIZn"},"source":["## Load the data and label"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5329,"status":"ok","timestamp":1727059189340,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"aahu3NcTMIZn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1330d8e9-9eac-402e-981a-ce21c367e000"},"outputs":[{"output_type":"stream","name":"stdout","text":["shape of train_data: (60000, 784)\n","shape of train_label: (60000,)\n"]}],"source":["train_load = np.loadtxt('./data/kmnist-train.csv',delimiter=',',dtype=\"int\")\n","train_data=train_load[:,1:]\n","train_label=train_load[:,0]\n","print(\"shape of train_data: {}\".format(train_data.shape))\n","print(\"shape of train_label: {}\".format(train_label.shape))"]},{"cell_type":"markdown","metadata":{"id":"RaauK7iDMIZo"},"source":["## Show the training data"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1727059189340,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"4ljhOuNKMIZo","scrolled":true},"outputs":[],"source":["# uncomment if you want to show the training data\n","#plt.figure(figsize=(20, 20))\n","#for index in range(10):\n","#    image = train_data[index+20000].reshape(28,28)\n","#    plt.subplot(2, 5, index+1)\n","#    plt.imshow(image)\n","#plt.show()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1727059189341,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"ll-Mmg5uMIZo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b3936f6-6f6c-4b0c-f4e6-fbc38f60168c"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_image_num  is : 60000\n"]}],"source":["train_image_num = train_data.shape[0]\n","train_data = train_data.astype('float32')\n","\n","print(\"train_image_num  is : {}\".format(train_image_num))"]},{"cell_type":"markdown","metadata":{"id":"wZcuiUhgnlPg"},"source":["## Change numpy array to pytorch tensor"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1727059189341,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"l65tjhk9njR8"},"outputs":[],"source":["train_data_tensor = torch.from_numpy(train_data)\n","train_label_tensor = torch.from_numpy(train_label)"]},{"cell_type":"markdown","metadata":{"id":"Lba_hUSCid9_"},"source":["## Validation image number"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1727059189341,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"ft6nsKeqMIZo"},"outputs":[],"source":["val_image_num=10000"]},{"cell_type":"markdown","metadata":{"id":"9fB4gFsigckg"},"source":["## Convert labels to one hot vector\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1727059189708,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"TlfYBY48gld_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed8bf0df-69e7-49aa-deea-495dec9cc7e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot training labels shape: (60000, 10)\n"]}],"source":["label_temp = np.zeros((train_image_num, 10), dtype = np.float32)\n","for i in range(train_image_num):\n","    label_temp[i][train_label[i]] = 1\n","train_label_onehot = np.copy(label_temp)\n","train_label_onehot_tensor = torch.from_numpy(train_label_onehot)\n","print(\"One-hot training labels shape:\",train_label_onehot.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"WgDMf7cUMIZp"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1727059189708,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"WDROhN4WMIZp"},"outputs":[],"source":["EPOCH = 200\n","Batch_size = 50 # 10000 should be divisible by batch_size\n","Learning_rate = 1e-3"]},{"cell_type":"markdown","metadata":{"id":"4EGZqeH-ULJK"},"source":["## Define the models with pytorch"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1727059189708,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"HRxsNpDkUKv-"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(1 * 28 * 28, 1024)  # Adjust input size to 1*28*28 for flattened images\n","        self.fc2 = nn.Linear(1024, 10)  # 10 output classes (for MNIST)\n","\n","    def forward(self, x):\n","        # Flatten input from (batch_size, 1, 28, 28) to (batch_size, 784)\n","        x = x.view(-1, 1 * 28 * 28)\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)  # Final layer without activation (softmax applied later if needed)\n","        return x\n","\n","# Instantiate the network\n","net = Net()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727059189708,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"q28N2EWu1I0a"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0OzokU54UVWM"},"source":["## Criterion and Optimizer"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1727059190772,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"},"user_tz":-480},"id":"5FggKYxKUUvK"},"outputs":[],"source":["import torch.optim as optim\n","\n","\n","criterion = nn.CrossEntropyLoss()  # Assuming classification task\n","optimizer = optim.SGD(net.parameters(), lr=Learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"j9Eii_nPMIZp"},"source":["## Training"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"irPDy1pZWMsz","executionInfo":{"status":"ok","timestamp":1727060243514,"user_tz":-480,"elapsed":1052745,"user":{"displayName":"陳緯亭","userId":"16426691216274961107"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d28840ff-6f0c-45d8-a079-f1940c860e10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Task2  | Epoch:  1  |Train Loss:  1.6340  |Train Acc:84.6820  |Val Loss:  0.4221  |Val Acc:89.8200\n","Task2  | Epoch:  2  |Train Loss:  0.2096  |Train Acc:94.5180  |Val Loss:  0.3484  |Val Acc:91.7600\n","Task2  | Epoch:  3  |Train Loss:  0.0931  |Train Acc:97.6040  |Val Loss:  0.3321  |Val Acc:92.2800\n","Task2  | Epoch:  4  |Train Loss:  0.0453  |Train Acc:99.0420  |Val Loss:  0.3297  |Val Acc:92.7200\n","Task2  | Epoch:  5  |Train Loss:  0.0238  |Train Acc:99.6360  |Val Loss:  0.3320  |Val Acc:92.8000\n","Task2  | Epoch:  6  |Train Loss:  0.0133  |Train Acc:99.8800  |Val Loss:  0.3323  |Val Acc:93.0600\n","Task2  | Epoch:  7  |Train Loss:  0.0085  |Train Acc:99.9600  |Val Loss:  0.3326  |Val Acc:93.3100\n","Task2  | Epoch:  8  |Train Loss:  0.0061  |Train Acc:99.9760  |Val Loss:  0.3351  |Val Acc:93.4000\n","Task2  | Epoch:  9  |Train Loss:  0.0047  |Train Acc:99.9860  |Val Loss:  0.3371  |Val Acc:93.4500\n","Task2  | Epoch: 10  |Train Loss:  0.0039  |Train Acc:99.9920  |Val Loss:  0.3389  |Val Acc:93.4900\n","Task2  | Epoch: 11  |Train Loss:  0.0033  |Train Acc:99.9960  |Val Loss:  0.3407  |Val Acc:93.5200\n","Task2  | Epoch: 12  |Train Loss:  0.0028  |Train Acc:100.0000  |Val Loss:  0.3423  |Val Acc:93.5900\n","Task2  | Epoch: 13  |Train Loss:  0.0025  |Train Acc:100.0000  |Val Loss:  0.3439  |Val Acc:93.6600\n","Task2  | Epoch: 14  |Train Loss:  0.0023  |Train Acc:100.0000  |Val Loss:  0.3452  |Val Acc:93.6500\n","Task2  | Epoch: 15  |Train Loss:  0.0020  |Train Acc:100.0000  |Val Loss:  0.3465  |Val Acc:93.6800\n","Task2  | Epoch: 16  |Train Loss:  0.0019  |Train Acc:100.0000  |Val Loss:  0.3478  |Val Acc:93.6800\n","Task2  | Epoch: 17  |Train Loss:  0.0017  |Train Acc:100.0000  |Val Loss:  0.3490  |Val Acc:93.7400\n","Task2  | Epoch: 18  |Train Loss:  0.0016  |Train Acc:100.0000  |Val Loss:  0.3501  |Val Acc:93.7400\n","Task2  | Epoch: 19  |Train Loss:  0.0015  |Train Acc:100.0000  |Val Loss:  0.3511  |Val Acc:93.7400\n","Task2  | Epoch: 20  |Train Loss:  0.0014  |Train Acc:100.0000  |Val Loss:  0.3520  |Val Acc:93.7500\n","Task2  | Epoch: 21  |Train Loss:  0.0013  |Train Acc:100.0000  |Val Loss:  0.3529  |Val Acc:93.7600\n","Task2  | Epoch: 22  |Train Loss:  0.0012  |Train Acc:100.0000  |Val Loss:  0.3537  |Val Acc:93.8000\n","Task2  | Epoch: 23  |Train Loss:  0.0011  |Train Acc:100.0000  |Val Loss:  0.3546  |Val Acc:93.8400\n","Task2  | Epoch: 24  |Train Loss:  0.0011  |Train Acc:100.0000  |Val Loss:  0.3553  |Val Acc:93.8800\n","Task2  | Epoch: 25  |Train Loss:  0.0010  |Train Acc:100.0000  |Val Loss:  0.3561  |Val Acc:93.8600\n","Task2  | Epoch: 26  |Train Loss:  0.0010  |Train Acc:100.0000  |Val Loss:  0.3568  |Val Acc:93.8700\n","Task2  | Epoch: 27  |Train Loss:  0.0009  |Train Acc:100.0000  |Val Loss:  0.3575  |Val Acc:93.8700\n","Task2  | Epoch: 28  |Train Loss:  0.0009  |Train Acc:100.0000  |Val Loss:  0.3581  |Val Acc:93.8700\n","Task2  | Epoch: 29  |Train Loss:  0.0009  |Train Acc:100.0000  |Val Loss:  0.3587  |Val Acc:93.9000\n","Task2  | Epoch: 30  |Train Loss:  0.0008  |Train Acc:100.0000  |Val Loss:  0.3594  |Val Acc:93.9200\n","Task2  | Epoch: 31  |Train Loss:  0.0008  |Train Acc:100.0000  |Val Loss:  0.3599  |Val Acc:93.9500\n","Task2  | Epoch: 32  |Train Loss:  0.0008  |Train Acc:100.0000  |Val Loss:  0.3605  |Val Acc:93.9800\n","Task2  | Epoch: 33  |Train Loss:  0.0007  |Train Acc:100.0000  |Val Loss:  0.3611  |Val Acc:93.9800\n","Task2  | Epoch: 34  |Train Loss:  0.0007  |Train Acc:100.0000  |Val Loss:  0.3616  |Val Acc:93.9700\n","Task2  | Epoch: 35  |Train Loss:  0.0007  |Train Acc:100.0000  |Val Loss:  0.3621  |Val Acc:93.9900\n","Task2  | Epoch: 36  |Train Loss:  0.0007  |Train Acc:100.0000  |Val Loss:  0.3626  |Val Acc:93.9800\n","Task2  | Epoch: 37  |Train Loss:  0.0006  |Train Acc:100.0000  |Val Loss:  0.3631  |Val Acc:94.0100\n","Task2  | Epoch: 38  |Train Loss:  0.0006  |Train Acc:100.0000  |Val Loss:  0.3636  |Val Acc:94.0000\n","Task2  | Epoch: 39  |Train Loss:  0.0006  |Train Acc:100.0000  |Val Loss:  0.3641  |Val Acc:94.0100\n","Task2  | Epoch: 40  |Train Loss:  0.0006  |Train Acc:100.0000  |Val Loss:  0.3645  |Val Acc:94.0000\n","Task2  | Epoch: 41  |Train Loss:  0.0006  |Train Acc:100.0000  |Val Loss:  0.3650  |Val Acc:94.0100\n","Task2  | Epoch: 42  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3654  |Val Acc:94.0200\n","Task2  | Epoch: 43  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3659  |Val Acc:94.0400\n","Task2  | Epoch: 44  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3662  |Val Acc:94.0600\n","Task2  | Epoch: 45  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3666  |Val Acc:94.0700\n","Task2  | Epoch: 46  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3671  |Val Acc:94.0900\n","Task2  | Epoch: 47  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3674  |Val Acc:94.1100\n","Task2  | Epoch: 48  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3678  |Val Acc:94.1100\n","Task2  | Epoch: 49  |Train Loss:  0.0005  |Train Acc:100.0000  |Val Loss:  0.3682  |Val Acc:94.1100\n","Task2  | Epoch: 50  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3686  |Val Acc:94.1200\n","Task2  | Epoch: 51  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3689  |Val Acc:94.1200\n","Task2  | Epoch: 52  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3693  |Val Acc:94.1200\n","Task2  | Epoch: 53  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3696  |Val Acc:94.1300\n","Task2  | Epoch: 54  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3700  |Val Acc:94.1400\n","Task2  | Epoch: 55  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3703  |Val Acc:94.1500\n","Task2  | Epoch: 56  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3706  |Val Acc:94.1500\n","Task2  | Epoch: 57  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3709  |Val Acc:94.1500\n","Task2  | Epoch: 58  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3713  |Val Acc:94.1500\n","Task2  | Epoch: 59  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3716  |Val Acc:94.1600\n","Task2  | Epoch: 60  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3719  |Val Acc:94.1600\n","Task2  | Epoch: 61  |Train Loss:  0.0004  |Train Acc:100.0000  |Val Loss:  0.3721  |Val Acc:94.1700\n","Task2  | Epoch: 62  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3724  |Val Acc:94.1800\n","Task2  | Epoch: 63  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3727  |Val Acc:94.1700\n","Task2  | Epoch: 64  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3730  |Val Acc:94.1900\n","Task2  | Epoch: 65  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3733  |Val Acc:94.1800\n","Task2  | Epoch: 66  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3735  |Val Acc:94.1800\n","Task2  | Epoch: 67  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3738  |Val Acc:94.2000\n","Task2  | Epoch: 68  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3740  |Val Acc:94.2000\n","Task2  | Epoch: 69  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3743  |Val Acc:94.2100\n","Task2  | Epoch: 70  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3746  |Val Acc:94.2100\n","Task2  | Epoch: 71  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3748  |Val Acc:94.2100\n","Task2  | Epoch: 72  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3751  |Val Acc:94.2200\n","Task2  | Epoch: 73  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3753  |Val Acc:94.2500\n","Task2  | Epoch: 74  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3756  |Val Acc:94.2500\n","Task2  | Epoch: 75  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3758  |Val Acc:94.2400\n","Task2  | Epoch: 76  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3760  |Val Acc:94.2500\n","Task2  | Epoch: 77  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3762  |Val Acc:94.2700\n","Task2  | Epoch: 78  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3765  |Val Acc:94.2800\n","Task2  | Epoch: 79  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3767  |Val Acc:94.2700\n","Task2  | Epoch: 80  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3769  |Val Acc:94.2700\n","Task2  | Epoch: 81  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3771  |Val Acc:94.2900\n","Task2  | Epoch: 82  |Train Loss:  0.0003  |Train Acc:100.0000  |Val Loss:  0.3774  |Val Acc:94.2800\n","Task2  | Epoch: 83  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3776  |Val Acc:94.2800\n","Task2  | Epoch: 84  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3778  |Val Acc:94.2900\n","Task2  | Epoch: 85  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3780  |Val Acc:94.3100\n","Task2  | Epoch: 86  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3782  |Val Acc:94.3000\n","Task2  | Epoch: 87  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3784  |Val Acc:94.3000\n","Task2  | Epoch: 88  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3786  |Val Acc:94.3000\n","Task2  | Epoch: 89  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3788  |Val Acc:94.3000\n","Task2  | Epoch: 90  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3790  |Val Acc:94.3100\n","Task2  | Epoch: 91  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3792  |Val Acc:94.3100\n","Task2  | Epoch: 92  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3794  |Val Acc:94.3200\n","Task2  | Epoch: 93  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3796  |Val Acc:94.3300\n","Task2  | Epoch: 94  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3798  |Val Acc:94.3300\n","Task2  | Epoch: 95  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3800  |Val Acc:94.3300\n","Task2  | Epoch: 96  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3802  |Val Acc:94.3300\n","Task2  | Epoch: 97  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3803  |Val Acc:94.3300\n","Task2  | Epoch: 98  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3805  |Val Acc:94.3400\n","Task2  | Epoch: 99  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3807  |Val Acc:94.3400\n","Task2  | Epoch:100  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3809  |Val Acc:94.3400\n","Task2  | Epoch:101  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3811  |Val Acc:94.3400\n","Task2  | Epoch:102  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3812  |Val Acc:94.3500\n","Task2  | Epoch:103  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3814  |Val Acc:94.3400\n","Task2  | Epoch:104  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3816  |Val Acc:94.3500\n","Task2  | Epoch:105  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3818  |Val Acc:94.3500\n","Task2  | Epoch:106  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3819  |Val Acc:94.3500\n","Task2  | Epoch:107  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3821  |Val Acc:94.3500\n","Task2  | Epoch:108  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3823  |Val Acc:94.3500\n","Task2  | Epoch:109  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3824  |Val Acc:94.3500\n","Task2  | Epoch:110  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3826  |Val Acc:94.3400\n","Task2  | Epoch:111  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3828  |Val Acc:94.3400\n","Task2  | Epoch:112  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3829  |Val Acc:94.3400\n","Task2  | Epoch:113  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3831  |Val Acc:94.3400\n","Task2  | Epoch:114  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3832  |Val Acc:94.3400\n","Task2  | Epoch:115  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3834  |Val Acc:94.3400\n","Task2  | Epoch:116  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3835  |Val Acc:94.3500\n","Task2  | Epoch:117  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3837  |Val Acc:94.3500\n","Task2  | Epoch:118  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3838  |Val Acc:94.3600\n","Task2  | Epoch:119  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3840  |Val Acc:94.3600\n","Task2  | Epoch:120  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3841  |Val Acc:94.3600\n","Task2  | Epoch:121  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3843  |Val Acc:94.3600\n","Task2  | Epoch:122  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3844  |Val Acc:94.3600\n","Task2  | Epoch:123  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3846  |Val Acc:94.3600\n","Task2  | Epoch:124  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3847  |Val Acc:94.3700\n","Task2  | Epoch:125  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3849  |Val Acc:94.3700\n","Task2  | Epoch:126  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3850  |Val Acc:94.3700\n","Task2  | Epoch:127  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3852  |Val Acc:94.3700\n","Task2  | Epoch:128  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3853  |Val Acc:94.3700\n","Task2  | Epoch:129  |Train Loss:  0.0002  |Train Acc:100.0000  |Val Loss:  0.3855  |Val Acc:94.3700\n","Task2  | Epoch:130  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3856  |Val Acc:94.3700\n","Task2  | Epoch:131  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3857  |Val Acc:94.3800\n","Task2  | Epoch:132  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3859  |Val Acc:94.3700\n","Task2  | Epoch:133  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3860  |Val Acc:94.3700\n","Task2  | Epoch:134  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3861  |Val Acc:94.3700\n","Task2  | Epoch:135  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3863  |Val Acc:94.3700\n","Task2  | Epoch:136  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3864  |Val Acc:94.3700\n","Task2  | Epoch:137  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3865  |Val Acc:94.3700\n","Task2  | Epoch:138  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3867  |Val Acc:94.3700\n","Task2  | Epoch:139  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3868  |Val Acc:94.3700\n","Task2  | Epoch:140  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3869  |Val Acc:94.3700\n","Task2  | Epoch:141  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3871  |Val Acc:94.3700\n","Task2  | Epoch:142  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3872  |Val Acc:94.3700\n","Task2  | Epoch:143  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3873  |Val Acc:94.3700\n","Task2  | Epoch:144  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3874  |Val Acc:94.3700\n","Task2  | Epoch:145  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3876  |Val Acc:94.3700\n","Task2  | Epoch:146  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3877  |Val Acc:94.3700\n","Task2  | Epoch:147  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3878  |Val Acc:94.3800\n","Task2  | Epoch:148  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3879  |Val Acc:94.3900\n","Task2  | Epoch:149  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3880  |Val Acc:94.3900\n","Task2  | Epoch:150  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3882  |Val Acc:94.3900\n","Task2  | Epoch:151  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3883  |Val Acc:94.3900\n","Task2  | Epoch:152  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3884  |Val Acc:94.3900\n","Task2  | Epoch:153  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3885  |Val Acc:94.3900\n","Task2  | Epoch:154  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3886  |Val Acc:94.3900\n","Task2  | Epoch:155  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3888  |Val Acc:94.3900\n","Task2  | Epoch:156  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3889  |Val Acc:94.3900\n","Task2  | Epoch:157  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3890  |Val Acc:94.3900\n","Task2  | Epoch:158  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3891  |Val Acc:94.3900\n","Task2  | Epoch:159  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3892  |Val Acc:94.3900\n","Task2  | Epoch:160  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3893  |Val Acc:94.3900\n","Task2  | Epoch:161  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3894  |Val Acc:94.3900\n","Task2  | Epoch:162  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3896  |Val Acc:94.3900\n","Task2  | Epoch:163  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3897  |Val Acc:94.3900\n","Task2  | Epoch:164  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3898  |Val Acc:94.3900\n","Task2  | Epoch:165  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3899  |Val Acc:94.3900\n","Task2  | Epoch:166  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3900  |Val Acc:94.4000\n","Task2  | Epoch:167  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3901  |Val Acc:94.4000\n","Task2  | Epoch:168  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3902  |Val Acc:94.4100\n","Task2  | Epoch:169  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3903  |Val Acc:94.4100\n","Task2  | Epoch:170  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3904  |Val Acc:94.4100\n","Task2  | Epoch:171  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3905  |Val Acc:94.4100\n","Task2  | Epoch:172  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3906  |Val Acc:94.4100\n","Task2  | Epoch:173  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3907  |Val Acc:94.4100\n","Task2  | Epoch:174  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3909  |Val Acc:94.4100\n","Task2  | Epoch:175  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3910  |Val Acc:94.4100\n","Task2  | Epoch:176  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3911  |Val Acc:94.4100\n","Task2  | Epoch:177  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3912  |Val Acc:94.4100\n","Task2  | Epoch:178  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3913  |Val Acc:94.4100\n","Task2  | Epoch:179  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3914  |Val Acc:94.4100\n","Task2  | Epoch:180  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3915  |Val Acc:94.4100\n","Task2  | Epoch:181  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3916  |Val Acc:94.4100\n","Task2  | Epoch:182  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3917  |Val Acc:94.4200\n","Task2  | Epoch:183  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3918  |Val Acc:94.4200\n","Task2  | Epoch:184  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3919  |Val Acc:94.4200\n","Task2  | Epoch:185  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3920  |Val Acc:94.4200\n","Task2  | Epoch:186  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3921  |Val Acc:94.4300\n","Task2  | Epoch:187  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3922  |Val Acc:94.4300\n","Task2  | Epoch:188  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3923  |Val Acc:94.4300\n","Task2  | Epoch:189  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3924  |Val Acc:94.4300\n","Task2  | Epoch:190  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3925  |Val Acc:94.4300\n","Task2  | Epoch:191  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3926  |Val Acc:94.4400\n","Task2  | Epoch:192  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3927  |Val Acc:94.4400\n","Task2  | Epoch:193  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3928  |Val Acc:94.4400\n","Task2  | Epoch:194  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3928  |Val Acc:94.4400\n","Task2  | Epoch:195  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3929  |Val Acc:94.4400\n","Task2  | Epoch:196  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3930  |Val Acc:94.4400\n","Task2  | Epoch:197  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3931  |Val Acc:94.4400\n","Task2  | Epoch:198  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3932  |Val Acc:94.4400\n","Task2  | Epoch:199  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3933  |Val Acc:94.4400\n","Task2  | Epoch:200  |Train Loss:  0.0001  |Train Acc:100.0000  |Val Loss:  0.3934  |Val Acc:94.4400\n"]}],"source":["\n","train_batch_num = (train_image_num - val_image_num )//Batch_size\n","val_batch_num = (val_image_num)//Batch_size\n","\n","for epoch in range(1, EPOCH+1):\n","    train_hit = 0\n","    val_hit = 0\n","    total_train_loss = 0.0\n","    total_val_loss = 0.0\n","    for it in range(train_batch_num):\n","        optimizer.zero_grad()\n","        outputs = net(train_data_tensor[it*Batch_size:(it+1)*Batch_size])\n","        _, pred_index = torch.max(outputs, 1)\n","        train_hit += (pred_index == train_label_tensor[it*Batch_size:(it+1)*Batch_size]).sum().item()\n","        loss = criterion(outputs, train_label_onehot_tensor[it*Batch_size:(it+1)*Batch_size])\n","        loss.backward()\n","        optimizer.step()\n","        total_train_loss += loss.item()\n","\n","    with torch.no_grad():\n","        for titt in range(val_batch_num):\n","            tit=train_batch_num+titt\n","            outputs = net(train_data_tensor[tit*Batch_size:(tit+1)*Batch_size])\n","            _, pred_index = torch.max(outputs, 1)\n","            val_hit += (pred_index == train_label_tensor[tit*Batch_size:(tit+1)*Batch_size]).sum().item()\n","            loss = criterion(outputs, train_label_onehot_tensor[tit*Batch_size:(tit+1)*Batch_size])\n","            total_val_loss += loss.item()\n","\n","    print('Task2  | Epoch:%3d'%epoch, ' |Train Loss:%8.4f'%(total_train_loss/train_batch_num), ' |Train Acc:%3.4f'%(train_hit/(train_image_num-val_image_num)*100.0)\n","          , ' |Val Loss:%8.4f'%(total_val_loss/val_batch_num), ' |Val Acc:%3.4f'%(val_hit/val_image_num*100.0))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}