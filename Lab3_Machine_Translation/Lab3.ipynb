{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69eea4f8-2ce0-4b9b-a4ee-623f451b83b7",
   "metadata": {
    "id": "69eea4f8-2ce0-4b9b-a4ee-623f451b83b7"
   },
   "source": [
    "# Task description\n",
    "- Translate text from Chinese to English.\n",
    "- Main goal: Get familiar with transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091accb-f7b7-4209-9514-52be2cb5b283",
   "metadata": {
    "id": "0091accb-f7b7-4209-9514-52be2cb5b283"
   },
   "source": [
    "## install the required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ef449b-c921-42a5-a644-6cd78f11b8bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51ef449b-c921-42a5-a644-6cd78f11b8bf",
    "outputId": "e755f2b9-aea8-4341-f82d-a5b7c4e8e3f4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: packaging>17.1 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torchmetrics) (2.4.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torchmetrics) (0.11.7)\n",
      "Requirement already satisfied: setuptools in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: sympy in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cc633-1f74-475c-a748-d372a7330332",
   "metadata": {
    "id": "ca6cc633-1f74-475c-a748-d372a7330332"
   },
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702d9c8d-9bc2-4bbb-8251-3c861b90bf20",
   "metadata": {
    "id": "702d9c8d-9bc2-4bbb-8251-3c861b90bf20"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415f246-3192-454a-94c7-55681071539a",
   "metadata": {
    "id": "e415f246-3192-454a-94c7-55681071539a"
   },
   "source": [
    "## Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a05b8e-a4a1-4f56-b2be-7f2c36210ba3",
   "metadata": {
    "id": "28a05b8e-a4a1-4f56-b2be-7f2c36210ba3"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb849a-a87a-462d-ac8b-4dacb1183c9f",
   "metadata": {
    "id": "28bb849a-a87a-462d-ac8b-4dacb1183c9f"
   },
   "source": [
    "# Data\n",
    "- Original dataset is [20k-en-zh-translation-pinyin-hsk](https://huggingface.co/datasets/swaption2009/20k-en-zh-translation-pinyin-hsk)\n",
    "- We select 50000 English-Chinese sentence pairs for translation task\n",
    "\n",
    "- Args:\n",
    "  - BATCH_SIZE\n",
    "  - data_dir: the path to the given translation dataset\n",
    "- Tokenizer: BertTokenizer\n",
    "  - encode: convert text to token ID\n",
    "  - decode: convert token ID back to text\n",
    "- Add paddings\n",
    "  - make all the sentences the same length by inserting token ID = PAD_IDX at the back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Lo9hly6QpeHB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lo9hly6QpeHB",
    "outputId": "07413dbf-de23-4e07-9ac2-23fc06d760d7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379f6d70-9e58-4c43-a739-45da13f2738d",
   "metadata": {
    "id": "379f6d70-9e58-4c43-a739-45da13f2738d"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./translation_data.json\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdb5b1-fb5d-4f26-97a1-b19dea9c9254",
   "metadata": {
    "id": "27fdb5b1-fb5d-4f26-97a1-b19dea9c9254"
   },
   "source": [
    "## Show the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7de47d-cf72-4915-b700-31f45ffe14d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "cf7de47d-cf72-4915-b700-31f45ffe14d0",
    "outputId": "4d1e59a5-a711-47e7-ee39-3bcbe542ae64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slowly and not without struggle, America began...</td>\n",
       "      <td>美国缓慢地开始倾听，但并非没有艰难曲折。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dithering is a technique that blends your colo...</td>\n",
       "      <td>抖动是关于颜色混合的技术，使你的作品看起来更圆滑，或者只是创作有趣的材质。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This paper discusses the petrologic characteri...</td>\n",
       "      <td>本文以珲春早第三纪含煤盆地的地质构违背景为依据，分析了煤系地层的岩石学特征。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The second encounter relates to my grandfather...</td>\n",
       "      <td>第二次事件跟我爷爷的宝贝匣子有关。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One way to address these challenges would be t...</td>\n",
       "      <td>解决这些挑战的途径包括依照麻瓜在南非的经验设立真相与和解委员会。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>You were too obtuse to take the hint.</td>\n",
       "      <td>你太迟钝了， 没有理解这种暗示。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Therefore, in the event the mortgagee of ship ...</td>\n",
       "      <td>因此，在这种情况下船舶抵押权人放弃了债务人提供的担保就会影响其他担保人的利益，导致抵押权人的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Fourth, puncture administrative bloat.</td>\n",
       "      <td>第四，削弱行政膨胀。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Massimo Oddo says he won't be thinking about h...</td>\n",
       "      <td>马西莫。奥多声明他不会在世界杯决赛圈比赛结束之前考虑未来的俱乐部。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The observing mortals' statue The uncompleted ...</td>\n",
       "      <td>《冷眼观俗尘》 尚未完工的佛祖雕像，超凡脱俗的神情，似乎在禅悟街巷里匆忙行人的百态人生。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english  \\\n",
       "0      Slowly and not without struggle, America began...   \n",
       "1      Dithering is a technique that blends your colo...   \n",
       "2      This paper discusses the petrologic characteri...   \n",
       "3      The second encounter relates to my grandfather...   \n",
       "4      One way to address these challenges would be t...   \n",
       "...                                                  ...   \n",
       "49995              You were too obtuse to take the hint.   \n",
       "49996  Therefore, in the event the mortgagee of ship ...   \n",
       "49997             Fourth, puncture administrative bloat.   \n",
       "49998  Massimo Oddo says he won't be thinking about h...   \n",
       "49999  The observing mortals' statue The uncompleted ...   \n",
       "\n",
       "                                                 chinese  \n",
       "0                                   美国缓慢地开始倾听，但并非没有艰难曲折。  \n",
       "1                  抖动是关于颜色混合的技术，使你的作品看起来更圆滑，或者只是创作有趣的材质。  \n",
       "2                 本文以珲春早第三纪含煤盆地的地质构违背景为依据，分析了煤系地层的岩石学特征。  \n",
       "3                                      第二次事件跟我爷爷的宝贝匣子有关。  \n",
       "4                       解决这些挑战的途径包括依照麻瓜在南非的经验设立真相与和解委员会。  \n",
       "...                                                  ...  \n",
       "49995                                   你太迟钝了， 没有理解这种暗示。  \n",
       "49996  因此，在这种情况下船舶抵押权人放弃了债务人提供的担保就会影响其他担保人的利益，导致抵押权人的...  \n",
       "49997                                         第四，削弱行政膨胀。  \n",
       "49998                  马西莫。奥多声明他不会在世界杯决赛圈比赛结束之前考虑未来的俱乐部。  \n",
       "49999       《冷眼观俗尘》 尚未完工的佛祖雕像，超凡脱俗的神情，似乎在禅悟街巷里匆忙行人的百态人生。  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_raw_data = pd.read_json(data_dir)\n",
    "translation_raw_data = translation_raw_data\n",
    "display(translation_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87a32c-a292-4fe5-b8c5-9da3f429a23f",
   "metadata": {
    "id": "5d87a32c-a292-4fe5-b8c5-9da3f429a23f"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f78e839-f926-4610-b4f5-4943ebbbc2e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f78e839-f926-4610-b4f5-4943ebbbc2e5",
    "outputId": "156d1ef2-32b6-48c8-f0f3-5389b3b29064"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer_cn = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12bd1f4-8fd0-4014-be51-8935cff740d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c12bd1f4-8fd0-4014-be51-8935cff740d3",
    "outputId": "d25f2ae7-99ef-4909-a54a-a831cb6e4043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokenize length: 128\n"
     ]
    }
   ],
   "source": [
    "english_seqs = translation_raw_data[\"english\"].apply(lambda x: tokenizer_en.encode(x, add_special_tokens=True, padding=False))\n",
    "chinese_seqs = translation_raw_data[\"chinese\"].apply(lambda x: tokenizer_cn.encode(x, add_special_tokens=True, padding=False))\n",
    "\n",
    "MAX_TOKENIZE_LENGTH = max(english_seqs.str.len().max(),chinese_seqs.str.len().max()) # longest string\n",
    "MAX_TOKENIZE_LENGTH = pow(2, math.ceil(math.log(MAX_TOKENIZE_LENGTH)/math.log(2)))   # closest upper to the power of 2\n",
    "\n",
    "print(\"max tokenize length:\", MAX_TOKENIZE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b85b6c-152a-4883-9265-41efb22f9653",
   "metadata": {
    "id": "a7b85b6c-152a-4883-9265-41efb22f9653"
   },
   "source": [
    "## Add paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3169cc-328f-4f41-8905-951a6282edef",
   "metadata": {
    "id": "0c3169cc-328f-4f41-8905-951a6282edef"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "BOS_IDX = chinese_seqs.iloc[0][0]\n",
    "EOS_IDX = chinese_seqs.iloc[0][-1]\n",
    "\n",
    "def add_padding(token_list, max_length):\n",
    "    ### TO-DO: Add padding to make all the sentence the same length\n",
    "    # Add your logic here to pad the token_list to max_length\n",
    "    # This is just a placeholder, you need to implement the actual padding logic\n",
    "    padded_list = token_list + [PAD_IDX] * (max_length - len(token_list))\n",
    "    return padded_list # Return the padded token list\n",
    "\n",
    "chinese_seqs = chinese_seqs.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH))\n",
    "english_seqs = english_seqs.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ea75e9-8d80-4735-a7c7-7bc476eeb4f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7ea75e9-8d80-4735-a7c7-7bc476eeb4f6",
    "outputId": "b64fee9f-4516-42ae-d0c1-9e648d19120d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====chinese tokenized data=====\n",
      "[101, 5401, 1744, 5353, 2714, 1765, 2458, 1993, 967, 1420, 8024, 852, 2400, 7478, 3766, 3300, 5680, 7410, 3289, 2835, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "=====english tokenized data=====\n",
      "[101, 13060, 1105, 1136, 1443, 5637, 117, 1738, 1310, 1106, 5113, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# check the padding result\n",
    "print(\"=====chinese tokenized data=====\")\n",
    "print(chinese_seqs.iloc[0])\n",
    "\n",
    "print(\"=====english tokenized data=====\")\n",
    "print(english_seqs.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e757b-45d9-4be7-b852-4fcfef6de1cf",
   "metadata": {
    "id": "de3e757b-45d9-4be7-b852-4fcfef6de1cf"
   },
   "source": [
    "## Datalodader\n",
    "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
    "- Create dataloader to iterate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d3beb8a-ff48-46bd-9d1f-388580971d16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d3beb8a-ff48-46bd-9d1f-388580971d16",
    "outputId": "f4b053d1-600e-4f63-b29e-cd252c2ea40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 45000\n",
      "test_size: 5000\n"
     ]
    }
   ],
   "source": [
    "data_size = len(translation_raw_data)\n",
    "train_size = int(0.9*data_size)\n",
    "test_size = data_size - train_size\n",
    "print(\"train_size:\",train_size)\n",
    "print(\"test_size:\",test_size)\n",
    "\n",
    "en_training_data = []\n",
    "cn_training_data = []\n",
    "en_testing_data = []\n",
    "cn_testing_data = []\n",
    "\n",
    "for i in range(data_size):\n",
    "    if (i < train_size):\n",
    "        en_training_data.append(torch.Tensor(english_seqs.iloc[i]))\n",
    "        cn_training_data.append(torch.Tensor(chinese_seqs.iloc[i]))\n",
    "    else:\n",
    "        en_testing_data.append(torch.Tensor(english_seqs.iloc[i]))\n",
    "        cn_testing_data.append(torch.Tensor(chinese_seqs.iloc[i]))\n",
    "\n",
    "\n",
    "class TextTranslationDataset(Dataset):\n",
    "    def __init__(self, src, dst):\n",
    "        self.src_list = src\n",
    "        self.dst_list = dst\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_list[idx], self.dst_list[idx]\n",
    "\n",
    "cn_to_en_train_set = TextTranslationDataset(cn_training_data, en_training_data)\n",
    "cn_to_en_test_set = TextTranslationDataset(cn_testing_data, en_testing_data)\n",
    "\n",
    "cn_to_en_train_loader = DataLoader(cn_to_en_train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cn_to_en_test_loader = DataLoader(cn_to_en_test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03160db-a2ab-49a2-a538-344a8f27c89c",
   "metadata": {
    "id": "a03160db-a2ab-49a2-a538-344a8f27c89c"
   },
   "source": [
    "# Model\n",
    "- TO-DO: Finish the model by yourself\n",
    "- Base transformer layers in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "    - TransformerEncoderLayer:\n",
    "    - TransformerDecoderLayer:\n",
    "- Positional encoding and input embedding\n",
    "- Note that you may need masks when implementing attention mechanism\n",
    "    - Padding mask: prevent input from attending to padding tokens\n",
    "    - Causal mask: prevent decoder input from attending to future input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4e36b6-c529-4b65-94fa-eed6a8a6cb52",
   "metadata": {
    "id": "9f4e36b6-c529-4b65-94fa-eed6a8a6cb52"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_k = d_model // nhead\n",
    "\n",
    "        # Linear layers for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final output linear layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, src_padding_mask=None, future_mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        matmul_qk = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        matmul_qk = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "\n",
    "        # Add future mask for preventing access to future tokens in decoder\n",
    "        if future_mask is not None:\n",
    "            matmul_qk += future_mask\n",
    "        # Apply source padding mask if present\n",
    "        if src_padding_mask is not None:\n",
    "            matmul_qk = matmul_qk.masked_fill(src_padding_mask.unsqueeze(1).unsqueeze(2) == 1, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(matmul_qk, dim=-1)  # Attention weights\n",
    "        output = torch.matmul(attn, V)    # Weighted sum of values based on attention\n",
    "\n",
    "        # # Print intermediate values for debugging\n",
    "        # print(f\"QK: {matmul_qk}\")\n",
    "        # print(f\"Attention Weights: {attn}\")\n",
    "        # print(f\"Attention Output: {output}\")\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "    def forward(self, Q, K, V, key_padding_mask=None, attn_mask=None):\n",
    "        # Change input shapes to [batch_size, seq_len, d_model]\n",
    "        Q = Q.permute(1, 0, 2)\n",
    "        K = K.permute(1, 0, 2)\n",
    "        V = V.permute(1, 0, 2)\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Apply linear transformations to Q, K, V and split into heads\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # # Print Q, K, V after projection and split into heads\n",
    "        # print(f\"Q (after projection and split): {Q}\")\n",
    "        # print(f\"K (after projection and split): {K}\")\n",
    "        # print(f\"V (after projection and split): {V}\")\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, key_padding_mask, attn_mask)\n",
    "\n",
    "        # Concatenate attention heads and apply the final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        attn_output = self.W_o(attn_output)\n",
    "\n",
    "        # # Print final output after concatenating heads and applying output linear layer\n",
    "        # print(f\"Final Output (before permute): {attn_output}\")\n",
    "\n",
    "        # Revert to original shape [seq_len, batch_size, d_model]\n",
    "        attn_output = attn_output.permute(1, 0, 2)\n",
    "\n",
    "        # print(f\"Final Output (after permute): {attn_output}\")\n",
    "\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ecfe90-37fa-4735-8265-2dcada1d819e",
   "metadata": {
    "id": "36ecfe90-37fa-4735-8265-2dcada1d819e"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, nhead, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # Custom Multi-Head Attention mechanism\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, nhead=nhead)\n",
    "\n",
    "        # Feedforward network (two linear layers with ReLU activation in between)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),  # Project from d_model to dim_feedforward\n",
    "            nn.ReLU(),                            # Apply ReLU activation\n",
    "            nn.Linear(dim_feedforward, d_model)    # Project back to d_model size\n",
    "        )\n",
    "\n",
    "        # Layer normalization to stabilize training\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_padding_mask=None):\n",
    "        # Self-attention block\n",
    "        attn_output = self.self_attn(x, x, x, src_padding_mask)  # Apply multi-head self-attention\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # Add & normalize (Residual connection + LayerNorm)\n",
    "\n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(x)  # Apply feedforward network\n",
    "        x = self.norm2(x + self.dropout2(ff_output))  # Add & normalize (Residual connection + LayerNorm)\n",
    "\n",
    "        return x  # Return the processed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e91586-43f3-4bb6-bdf2-989fca5e17d3",
   "metadata": {
    "id": "72e91586-43f3-4bb6-bdf2-989fca5e17d3"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, nhead, dropout):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        # MultiHeadAttention for self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, nhead=nhead)\n",
    "        self.cross_attn = MultiHeadAttention(d_model=d_model, nhead=nhead)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def residual_norm(self, x, sublayer_output, norm_layer):\n",
    "        \"\"\"Helper function for residual connection, dropout, and normalization.\"\"\"\n",
    "        return norm_layer(x + self.dropout(sublayer_output))\n",
    "\n",
    "    def forward(self, x, enc_output, src_padding_mask=None, tgt_padding_mask=None, tgt_future_mask=None):\n",
    "        # Self-attention with future masking (target sequence)\n",
    "        tgt_attn_output = self.self_attn(x, x, x, tgt_padding_mask, tgt_future_mask)\n",
    "        x = self.residual_norm(x, tgt_attn_output, self.norm1)\n",
    "\n",
    "        # Cross-attention (encoder-decoder attention)\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, key_padding_mask=src_padding_mask)\n",
    "        x = self.residual_norm(x, cross_attn_output, self.norm2)\n",
    "\n",
    "        # Feedforward neural network\n",
    "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        x = self.residual_norm(x, ff_output, self.norm3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0292eb1d-c796-4f6d-8db9-61082b6ec4b5",
   "metadata": {
    "id": "0292eb1d-c796-4f6d-8db9-61082b6ec4b5"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Final linear layer to map the Transformer's output to the vocabulary distribution\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, src_embeded, tgt_embeded, src_padding_mask, tgt_padding_mask, tgt_future_mask):\n",
    "        # 1. Pass the source sequence through the encoder\n",
    "        enc_output = self.encode(src_embeded, src_padding_mask)\n",
    "        # 2. Pass the target sequence and encoder output through the decoder\n",
    "        output = self.decode(tgt_embeded, enc_output, src_padding_mask, tgt_padding_mask, tgt_future_mask)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src_embeded, src_padding_mask=None):\n",
    "        \"\"\"Pass the embedded source sequence through all encoder layers.\"\"\"\n",
    "        for layer in self.encoder_layers:\n",
    "            src_embeded = layer(src_embeded, src_padding_mask)\n",
    "        return src_embeded\n",
    "\n",
    "    def decode(self, tgt_embeded, enc_output, src_padding_mask=None, tgt_padding_mask=None, tgt_future_mask=None):\n",
    "        \"\"\"Pass the embedded target sequence and encoder output through all decoder layers.\"\"\"\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt_embeded = layer(tgt_embeded, enc_output, src_padding_mask, tgt_padding_mask, tgt_future_mask)\n",
    "        return tgt_embeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6215f72e-6dfb-4a03-829c-56174f1e02bc",
   "metadata": {
    "id": "6215f72e-6dfb-4a03-829c-56174f1e02bc"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dropout, maxlen):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Calculate positional encodings\n",
    "        pos_enc = torch.zeros(maxlen, emb_size)\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * -(math.log(10000.0) / emb_size))\n",
    "\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)  # Use sin for even positions\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)  # Use cos for odd positions\n",
    "        pos_enc = pos_enc.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        self.register_buffer('pos_enc', pos_enc)  # Register positional encodings as a buffer (not trainable)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        # Add positional encodings to the token embeddings\n",
    "        return self.dropout(token_embedding + self.pos_enc[:, :token_embedding.size(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xDwHSFtRRzqB",
   "metadata": {
    "id": "xDwHSFtRRzqB"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)  # Create the embedding layer\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # Convert token indices to embeddings and scale by the square root of the embedding size\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ffec74",
   "metadata": {
    "id": "e9ffec74"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0862f43e-2d3b-4b4e-8e10-a4682dafd1ea",
   "metadata": {
    "id": "0862f43e-2d3b-4b4e-8e10-a4682dafd1ea"
   },
   "outputs": [],
   "source": [
    "def create_mask(src, tgt, pad_idx=PAD_IDX):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_future_mask = generate_square_subsequent_mask(tgt_seq_len) # Pass device to generate_square_subsequent_mask\n",
    "    # Create padding mask for source sequence\n",
    "    src_padding_mask = (src == pad_idx).transpose(0, 1)\n",
    "    # Create padding mask for target sequence\n",
    "    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1)\n",
    "                                                                                        # and ensure the mask is a boolean tensor\n",
    "    # If tgt_future_mask is provided, it will be used directly\n",
    "    src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "    tgt_padding_mask = tgt_padding_mask.to(DEVICE)\n",
    "    tgt_future_mask = tgt_future_mask.to(DEVICE)\n",
    "    return tgt_future_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fefeaad8-0546-48a6-8e6a-2b9881bc0899",
   "metadata": {
    "id": "fefeaad8-0546-48a6-8e6a-2b9881bc0899"
   },
   "outputs": [],
   "source": [
    "# Seq2Seq Network\n",
    "class Seq2SeqNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 emb_size,\n",
    "                 nhead,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 dim_feedforward,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            num_heads=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            d_ff=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        maxlen=5000\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout, maxlen=maxlen)\n",
    "\n",
    "    def forward(self,\n",
    "                src,\n",
    "                trg,\n",
    "                tgt_future_mask=None,\n",
    "                src_padding_mask=None,\n",
    "                tgt_padding_mask=None):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_future_mask=tgt_future_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "\n",
    "    def encode(self, src, src_padding_mask=None):\n",
    "        return self.transformer.encode(self.positional_encoding(self.src_tok_emb(src)), src_padding_mask=src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, src_padding_mask=None, tgt_padding_mask=None, tgt_future_mask=None):\n",
    "        return self.transformer.decode(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_future_mask=tgt_future_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36f176-95be-4ea2-b96d-694123fce4d7",
   "metadata": {
    "id": "da36f176-95be-4ea2-b96d-694123fce4d7"
   },
   "source": [
    "## Note: The parameter size of model should be less than 100M (100,000k) !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8473468-4332-461e-bd1b-e404730bf7d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8473468-4332-461e-bd1b-e404730bf7d8",
    "outputId": "69a0b21b-b8c3-480f-d50d-e7c190ff1f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameter size of transformer is 10898.884 k\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 128\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 1024\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "SRC_VOCAB_SIZE = tokenizer_cn.vocab_size\n",
    "TGT_VOCAB_SIZE = tokenizer_en.vocab_size\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformer = Seq2SeqNetwork(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "param_transformer = sum(p.numel() for p in transformer.parameters())\n",
    "print (f\"The parameter size of transformer is {param_transformer/1000} k\")\n",
    "#   The parameter size of model should be less than 100M (100,000k) !!!\n",
    "#   The parameter size of model should be less than 100M (100,000k) !!!\n",
    "#   The parameter size of model should be less than 100M (100,000k) !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8086b4f-57b4-4d8a-812b-03b50319a368",
   "metadata": {
    "id": "f8086b4f-57b4-4d8a-812b-03b50319a368"
   },
   "source": [
    "# Training\n",
    "- You can change the training setting by yourself including\n",
    "  - Number of epoch\n",
    "  - Optimizer\n",
    "  - Learning rate\n",
    "  - Learning rate scheduler\n",
    "  - etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409a5008-c773-484a-89e2-3670cc872f60",
   "metadata": {
    "id": "409a5008-c773-484a-89e2-3670cc872f60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NFS/opt/miniforge3/envs/dl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "LR = 0.001\n",
    "BETAS = (0.9, 0.98)\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR, betas=BETAS, eps=EPSILON)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8960b29-670f-43eb-bd14-9f78366d8080",
   "metadata": {
    "id": "a8960b29-670f-43eb-bd14-9f78366d8080"
   },
   "source": [
    "## Translation quality metrics: BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81785871-1002-45d0-9da0-2b002c12ad05",
   "metadata": {
    "id": "81785871-1002-45d0-9da0-2b002c12ad05"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "def bleu_score_func(predicted, truth, grams=1):\n",
    "    preds = [predicted]\n",
    "    truth = [[truth]]\n",
    "    bleu = BLEUScore(n_gram=grams)\n",
    "    return bleu(preds, truth)\n",
    "\n",
    "\n",
    "def BLEU_batch(predict, truth, output_tokenizer):\n",
    "    batch_size = predict.size(1)\n",
    "    total_score = 0\n",
    "    for i in range(batch_size):\n",
    "        predict_str = output_tokenizer.decode(predict[:, i], skip_special_tokens=True)\n",
    "        truth_str = output_tokenizer.decode(truth[:, i], skip_special_tokens=True)\n",
    "        score_gram1 = bleu_score_func(predict_str.lower(), truth_str.lower(), grams=1)\n",
    "        #score_gram2 = bleu_score_func(predict_str.lower(), truth_str, grams=2)\n",
    "        #score_gram3 = bleu_score_func(predict_str.lower(), truth_str, grams=3)\n",
    "        #score_gram4 = bleu_score_func(predict_str.lower(), truth_str, grams=4)\n",
    "        #total_score = total_score + (score_gram1 + score_gram2 + score_gram3 + score_gram4) / 4.0\n",
    "        total_score = total_score + score_gram1\n",
    "    total_score = total_score / batch_size\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9aa45c-886a-48bd-9f3b-a89a1d3c91aa",
   "metadata": {
    "id": "0b9aa45c-886a-48bd-9f3b-a89a1d3c91aa"
   },
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6813f99a-62a5-43d5-bc20-96cf9384acfe",
   "metadata": {
    "id": "6813f99a-62a5-43d5-bc20-96cf9384acfe"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_dataloader):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        # Prepare source and target inputs\n",
    "        src = src.transpose(0, 1).to(DEVICE).type(torch.long)\n",
    "        tgt = tgt.transpose(0, 1).to(DEVICE).type(torch.long)\n",
    "\n",
    "        # Prepare target input (remove last token) and target output (remove first token)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        # Create masks\n",
    "        tgt_future_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, PAD_IDX)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_future_mask=tgt_future_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1).long())\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return average loss per batch\n",
    "    return total_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "jnTCYaqcUgJZ",
   "metadata": {
    "id": "jnTCYaqcUgJZ"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    score = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.transpose(0, 1)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "\n",
    "        src = src.to(DEVICE).type(torch.long)\n",
    "        tgt = tgt.to(DEVICE).type(torch.long)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_future_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, PAD_IDX)\n",
    "\n",
    "        logits = model(src, tgt_input, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_future_mask=tgt_future_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        _, tgt_predict = torch.max(logits, dim=-1)\n",
    "        score_batch = BLEU_batch(tgt_predict, tgt_out, tokenizer_en)\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1).long())\n",
    "        losses += loss.item()\n",
    "        score += score_batch\n",
    "\n",
    "    # Return the average loss value and average BLEU score\n",
    "    return (losses / len(list(val_dataloader))), (score / len(list(val_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3012a-bd6f-4ca1-9ff5-ea1162920e0e",
   "metadata": {
    "id": "6bf3012a-bd6f-4ca1-9ff5-ea1162920e0e"
   },
   "source": [
    "## Start training\n",
    "- MODEL_SAVE_PATH: path for storing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2640ed86-192a-4a85-82da-46347fb765fb",
   "metadata": {
    "id": "2640ed86-192a-4a85-82da-46347fb765fb"
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"./model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a94b00cb-8ae1-427b-96c4-7b1b9bb840b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a94b00cb-8ae1-427b-96c4-7b1b9bb840b2",
    "outputId": "0b2d5ebf-dae4-461c-9fb9-8b20225b0e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 6.385, Val loss: 5.799, Val Acc: 0.157, Epoch time = 58.195s\n",
      "(model saved)\n",
      "Epoch: 2, Train loss: 5.542, Val loss: 5.481, Val Acc: 0.183, Epoch time = 58.579s\n",
      "(model saved)\n",
      "Epoch: 3, Train loss: 5.165, Val loss: 5.355, Val Acc: 0.188, Epoch time = 58.685s\n",
      "(model saved)\n",
      "Epoch: 4, Train loss: 4.894, Val loss: 5.313, Val Acc: 0.195, Epoch time = 58.728s\n",
      "(model saved)\n",
      "Epoch: 5, Train loss: 4.686, Val loss: 5.289, Val Acc: 0.201, Epoch time = 58.790s\n",
      "(model saved)\n",
      "Epoch: 6, Train loss: 4.520, Val loss: 5.301, Val Acc: 0.204, Epoch time = 58.818s\n",
      "(model saved)\n",
      "Epoch: 7, Train loss: 4.385, Val loss: 5.321, Val Acc: 0.204, Epoch time = 58.846s\n",
      "Epoch: 8, Train loss: 4.278, Val loss: 5.352, Val Acc: 0.205, Epoch time = 58.994s\n",
      "(model saved)\n",
      "Epoch: 9, Train loss: 4.026, Val loss: 5.347, Val Acc: 0.212, Epoch time = 58.974s\n",
      "(model saved)\n",
      "Epoch: 10, Train loss: 3.947, Val loss: 5.372, Val Acc: 0.215, Epoch time = 58.957s\n",
      "(model saved)\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, cn_to_en_train_loader)\n",
    "    end_time = timer()\n",
    "    val_loss, val_acc = evaluate(transformer, cn_to_en_test_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "    # Save the best model so far.\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_state_dict = transformer.state_dict()\n",
    "        torch.save(best_state_dict, MODEL_SAVE_PATH)\n",
    "        print(\"(model saved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c0699-b80e-40eb-b7b0-0906b09784c0",
   "metadata": {
    "id": "fe1c0699-b80e-40eb-b7b0-0906b09784c0"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47e06c7d-8ba4-4761-86f2-b7db7ed6b3a0",
   "metadata": {
    "id": "47e06c7d-8ba4-4761-86f2-b7db7ed6b3a0"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)  # Moving src tensor to the appropriate device\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_padding_mask=src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_padding_mask = torch.ones(1, ys.size(0)).type(torch.bool).to(DEVICE)  # target padding mask\n",
    "        tgt_padding_mask = ~tgt_padding_mask\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))).to(DEVICE)  # target causal mask\n",
    "        out = model.decode(ys, memory, src_padding_mask=src_mask, tgt_padding_mask=tgt_padding_mask, tgt_future_mask=tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a886b34-7f5d-4954-aafb-a22232ad92f9",
   "metadata": {
    "id": "6a886b34-7f5d-4954-aafb-a22232ad92f9"
   },
   "outputs": [],
   "source": [
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str, input_tokenizer, output_tokenizer):\n",
    "    model.eval()\n",
    "    sentence = input_tokenizer.encode(src_sentence)\n",
    "    sentence = torch.tensor(sentence).view(-1, 1)\n",
    "    num_tokens = sentence.shape[0]\n",
    "\n",
    "    src_mask = torch.ones(1, num_tokens).type(torch.bool)  # source padding mask\n",
    "    src_mask = ~src_mask\n",
    "    tgt_tokens = greedy_decode(model, sentence, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    output_sentence = output_tokenizer.decode(tgt_tokens, skip_special_tokens=True)\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41175b6f-7fb5-48d2-bc7c-3b9cff764d17",
   "metadata": {
    "id": "41175b6f-7fb5-48d2-bc7c-3b9cff764d17"
   },
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3198b31-30a3-46aa-bb02-22913de7909e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3198b31-30a3-46aa-bb02-22913de7909e",
    "outputId": "2f31bf0b-976a-4280-e8e6-00b1e7324152"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_168139/2163959858.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer.load_state_dict(torch.load(\"model.ckpt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Seq2SeqNetwork(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer.to(DEVICE)\n",
    "transformer.load_state_dict(torch.load(\"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560658f5-a89e-464b-8e08-8955dafc68c0",
   "metadata": {
    "id": "560658f5-a89e-464b-8e08-8955dafc68c0"
   },
   "source": [
    "## Translation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a44dc1b7-38be-4692-9981-dea8845dd9d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a44dc1b7-38be-4692-9981-dea8845dd9d3",
    "outputId": "6d7df3ee-565e-41c2-8879-99982d19f955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : 你好，欢迎来到中国\n",
      "Prediction     : You are going to welcome you.\n",
      "Ground truth   : Hello, Welcome to China\n",
      "Bleu Score (1gram):  0.3333333134651184\n",
      "Bleu Score (2gram):  0.0\n",
      "Bleu Score (3gram):  0.0\n",
      "Bleu Score (4gram):  0.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"你好，欢迎来到中国\"\n",
    "ground_truth = 'Hello, Welcome to China'\n",
    "predicted = translate(transformer, sentence, tokenizer_cn, tokenizer_en)\n",
    "\n",
    "print(f'{\"Input:\":15s}: {sentence}')\n",
    "print(f'{\"Prediction\":15s}: {predicted}')\n",
    "print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "print(\"Bleu Score (1gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 1).item())\n",
    "print(\"Bleu Score (2gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 2).item())\n",
    "print(\"Bleu Score (3gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 3).item())\n",
    "print(\"Bleu Score (4gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b7ed8be-73ba-441c-8490-3ad93ead9f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b7ed8be-73ba-441c-8490-3ad93ead9f91",
    "outputId": "6b74d67b-d288-4b72-d990-b1aa4bb2ebed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : 早上好，很高心见到你\n",
      "Prediction     : You see the early morning, you see.\n",
      "Ground truth   : Good Morning, nice to meet you\n",
      "Bleu Score (1gram):  0.2857142984867096\n",
      "Bleu Score (2gram):  0.0\n",
      "Bleu Score (3gram):  0.0\n",
      "Bleu Score (4gram):  0.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"早上好，很高心见到你\"\n",
    "ground_truth = 'Good Morning, nice to meet you'\n",
    "predicted = translate(transformer, sentence, tokenizer_cn, tokenizer_en)\n",
    "\n",
    "print(f'{\"Input:\":15s}: {sentence}')\n",
    "print(f'{\"Prediction\":15s}: {predicted}')\n",
    "print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "print(\"Bleu Score (1gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 1).item())\n",
    "print(\"Bleu Score (2gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 2).item())\n",
    "print(\"Bleu Score (3gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 3).item())\n",
    "print(\"Bleu Score (4gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12708039-9697-4918-b7f7-1dfa09cff76e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12708039-9697-4918-b7f7-1dfa09cff76e",
    "outputId": "fce9d055-3fbd-4c13-d779-2ce405c3373d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : 祝您有个美好的一天\n",
      "Prediction     : You have a good day.\n",
      "Ground truth   : Have a nice day\n",
      "Bleu Score (1gram):  0.4000000059604645\n",
      "Bleu Score (2gram):  0.3162277638912201\n",
      "Bleu Score (3gram):  0.0\n",
      "Bleu Score (4gram):  0.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"祝您有个美好的一天\"\n",
    "ground_truth = 'Have a nice day'\n",
    "predicted = translate(transformer, sentence, tokenizer_cn, tokenizer_en)\n",
    "\n",
    "print(f'{\"Input:\":15s}: {sentence}')\n",
    "print(f'{\"Prediction\":15s}: {predicted}')\n",
    "print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "print(\"Bleu Score (1gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 1).item())\n",
    "print(\"Bleu Score (2gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 2).item())\n",
    "print(\"Bleu Score (3gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 3).item())\n",
    "print(\"Bleu Score (4gram): \", bleu_score_func(predicted.lower(), ground_truth.lower(), 4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22ebbb-8c6a-4f53-8136-fcc64f2bb9b4",
   "metadata": {
    "id": "6e22ebbb-8c6a-4f53-8136-fcc64f2bb9b4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c156428-4267-4ecd-bfd4-c544801515a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
